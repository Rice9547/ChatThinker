import os
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()

class ChatProcessor:
    def __init__(self, session_manager=None):
        self.llm = ChatOpenAI(
            temperature=0.7,
            model="gpt-3.5-turbo",
            openai_api_key=os.getenv('OPENAI_API_KEY')
        )
        self.session_manager = session_manager
    
    def generate_conversation(self, session_data, user_id=None):
        """ç”Ÿæˆ3å€‹å¯ç›´æ¥ä½¿ç”¨çš„å›è¦†é¸é …"""
        
        # åˆ†æéå»å°è©±ï¼Œæ‰¾å‡ºéœ€è¦å›æ‡‰çš„é‡é»
        past_conv = session_data.get('past_conversation', 'ç„¡')
        
        # å¦‚æœæœ‰éå»å°è©±ï¼Œç‰¹åˆ¥å¼·èª¿è¦å›æ‡‰å°æ–¹çš„å•é¡Œ
        if past_conv != 'ç„¡' and 'ï¼Ÿ' in past_conv:
            context_instruction = """
            ç‰¹åˆ¥æ³¨æ„ï¼šå°æ–¹æå‡ºäº†å•é¡Œï¼Œä½ çš„å›è¦†å¿…é ˆç›´æ¥å›ç­”é€™å€‹å•é¡Œã€‚
            å¾éå»å°è©±ä¸­æ‰¾å‡ºå°æ–¹çš„ç–‘å•ï¼Œä¸¦åœ¨å›è¦†ä¸­çµ¦äºˆå…·é«”ç­”æ¡ˆã€‚
            """
        else:
            context_instruction = ""
        
        prompt_template = ChatPromptTemplate.from_template("""
        ä½ æ˜¯ä¸€å€‹å°ç£å°è©±å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šï¼Œç”Ÿæˆ3å€‹å¯ä»¥ç›´æ¥è¤‡è£½ä½¿ç”¨çš„å›è¦†è¨Šæ¯ã€‚

        æƒ…å¢ƒè³‡è¨Šï¼š
        - æˆ‘çš„èº«ä»½ï¼š{user_identity}
        - å°è©±å°è±¡ï¼š{target_identity}  
        - å°è©±æƒ…å¢ƒï¼š{context}
        - éå»å°è©±ï¼š{past_conversation}

        {context_instruction}

        é‡è¦è¦æ±‚ï¼š
        1. åªæä¾›å¯ä»¥ç›´æ¥å‚³é€çš„è¨Šæ¯å…§å®¹
        2. ä¸è¦åŒ…å«ä»»ä½•æ¨™ç±¤æˆ–èªªæ˜æ–‡å­—
        3. ä½¿ç”¨ç¹é«”ä¸­æ–‡èˆ‡å°ç£ç”¨èª
        4. å¦‚æœå°æ–¹æœ‰æå•ï¼Œå¿…é ˆå…·é«”å›ç­”
        5. ç¬¦åˆèº«ä»½èˆ‡æƒ…å¢ƒçš„èªæ°£

        è«‹æä¾›3å€‹ä¸åŒé¢¨æ ¼çš„å›è¦†ï¼š

        âœ… ç‰ˆæœ¬1ã€æ­£å¼å°ˆæ¥­ã€‘
        ï¼ˆæä¾›ä¸€å€‹æ­£å¼ä½†å‹å–„çš„å›è¦†ï¼Œé©åˆç¶­æŒå°ˆæ¥­å½¢è±¡ï¼‰

        âœ… ç‰ˆæœ¬2ã€å¹³è¡¡å‹å–„ã€‘  
        ï¼ˆæä¾›ä¸€å€‹å¹³è¡¡å°ˆæ¥­èˆ‡è¦ªåˆ‡çš„å›è¦†ï¼‰

        âœ… ç‰ˆæœ¬3ã€è¼•é¬†è¦ªåˆ‡ã€‘
        ï¼ˆæä¾›ä¸€å€‹è¼ƒè¼•é¬†ä½†ä»ç„¶å¾—é«”çš„å›è¦†ï¼‰

        è¨˜ä½ï¼šæ¯å€‹ç‰ˆæœ¬éƒ½è¦èƒ½ç›´æ¥è¤‡è£½è²¼ä¸Šä½¿ç”¨ï¼
        """)
        
        chain = prompt_template | self.llm
        
        last_prompt = {
            'user_identity': session_data.get('user_identity', ''),
            'target_identity': session_data.get('target_identity', ''),
            'context': session_data.get('context', ''),
            'past_conversation': session_data.get('past_conversation', 'ç„¡'),
            'context_instruction': context_instruction
        }
        
        if self.session_manager and user_id:
            self.session_manager.save_last_prompt(user_id, last_prompt)
        
        result = chain.invoke(last_prompt)
        
        # æ ¼å¼åŒ–è¼¸å‡ºï¼ŒåŠ ä¸Šåˆ†éš”ç·šè®“ç”¨æˆ¶æ›´å®¹æ˜“è¤‡è£½
        content = result.content.strip()
        
        # åŠ å…¥ä½¿ç”¨æç¤º
        formatted_output = "ğŸ“ ä»¥ä¸‹æ˜¯3å€‹å›è¦†é¸é …ï¼Œè«‹é¸æ“‡é©åˆçš„è¤‡è£½ä½¿ç”¨ï¼š\n\n"
        formatted_output += "=" * 40 + "\n"
        formatted_output += content
        formatted_output += "\n" + "=" * 40
        formatted_output += "\n\nğŸ’¡ å°æç¤ºï¼šç›´æ¥é•·æŒ‰è¨Šæ¯å³å¯è¤‡è£½\nè¼¸å…¥ /more å¯ç²å¾—æ›´å¤šç‰ˆæœ¬"
        
        return formatted_output
    
    def polish_conversation(self, session_data, draft, user_id=None):
        """å„ªåŒ–ä½¿ç”¨è€…æä¾›çš„è‰ç¨¿"""
        
        prompt_template = ChatPromptTemplate.from_template("""
        ä½ æ˜¯ä¸€å€‹å°ç£å°è©±å°ˆå®¶ã€‚è«‹å„ªåŒ–ä»¥ä¸‹è‰ç¨¿ï¼Œæä¾›3å€‹æ”¹é€²ç‰ˆæœ¬ã€‚

        æƒ…å¢ƒè³‡è¨Šï¼š
        - æˆ‘çš„èº«ä»½ï¼š{user_identity}
        - å°è©±å°è±¡ï¼š{target_identity}
        - å°è©±æƒ…å¢ƒï¼š{context}
        - éå»å°è©±ï¼š{past_conversation}

        ä½¿ç”¨è€…çš„è‰ç¨¿ï¼š
        ã€Œ{draft}ã€

        è«‹æä¾›3å€‹å„ªåŒ–ç‰ˆæœ¬ï¼Œæ¯å€‹éƒ½è¦èƒ½ç›´æ¥ä½¿ç”¨ï¼š

        âœ… ç‰ˆæœ¬1ã€æ­£å¼å°ˆæ¥­ã€‘
        ï¼ˆä¿æŒå°ˆæ¥­ä½†åŠ å…¥é©ç•¶çš„å‹å–„æ„Ÿï¼‰

        âœ… ç‰ˆæœ¬2ã€å¹³è¡¡å‹å–„ã€‘
        ï¼ˆå¹³è¡¡å°ˆæ¥­èˆ‡è¦ªåˆ‡ï¼Œæœ€å®‰å…¨çš„é¸æ“‡ï¼‰

        âœ… ç‰ˆæœ¬3ã€è¼•é¬†è¦ªåˆ‡ã€‘
        ï¼ˆè¼ƒå£èªä½†ä»ä¿æŒç¦®è²Œï¼‰

        å„ªåŒ–é‡é»ï¼š
        - ä¿ç•™åŸæ„ä½†æ”¹å–„è¡¨é”
        - æ›´è‡ªç„¶çš„å°ç£ç”¨èª
        - é©ç•¶çš„èªæ°£èª¿æ•´
        """)
        
        chain = prompt_template | self.llm
        
        last_prompt = {
            'user_identity': session_data.get('user_identity', ''),
            'target_identity': session_data.get('target_identity', ''),
            'context': session_data.get('context', ''),
            'past_conversation': session_data.get('past_conversation', 'ç„¡'),
            'draft': draft
        }
        
        if self.session_manager and user_id:
            self.session_manager.save_last_prompt(user_id, last_prompt)
        
        result = chain.invoke(last_prompt)
        
        formatted_output = "âœ¨ ä»¥ä¸‹æ˜¯å„ªåŒ–å¾Œçš„3å€‹ç‰ˆæœ¬ï¼š\n\n"
        formatted_output += "=" * 40 + "\n"
        formatted_output += result.content.strip()
        formatted_output += "\n" + "=" * 40
        formatted_output += "\n\nğŸ’¡ å°æç¤ºï¼šç›´æ¥é•·æŒ‰è¨Šæ¯å³å¯è¤‡è£½"
        
        return formatted_output
    
    def generate_more(self, last_prompt):
        """ç”Ÿæˆæ›´å¤šç‰ˆæœ¬"""
        if not last_prompt:
            return "æ²’æœ‰æ‰¾åˆ°ä¹‹å‰çš„å°è©±è¨˜éŒ„"
        
        # æ ¹æ“šæ˜¯å¦æœ‰è‰ç¨¿æ±ºå®šæç¤ºè©
        if 'draft' in last_prompt:
            task_description = "å„ªåŒ–è‰ç¨¿"
        else:
            task_description = "å›è¦†å°è©±"
        
        prompt_template = ChatPromptTemplate.from_template("""
        è«‹æ ¹æ“šç›¸åŒè³‡è¨Šï¼Œå†æä¾›3å€‹ä¸åŒé¢¨æ ¼çš„{task_description}ç‰ˆæœ¬ã€‚

        æƒ…å¢ƒè³‡è¨Šï¼š
        - æˆ‘çš„èº«ä»½ï¼š{user_identity}
        - å°è©±å°è±¡ï¼š{target_identity}
        - å°è©±æƒ…å¢ƒï¼š{context}
        - éå»å°è©±ï¼š{past_conversation}

        é€™æ¬¡è«‹å˜—è©¦ä¸åŒçš„è§’åº¦ï¼š

        âœ… ç‰ˆæœ¬4ã€æ›´å§”å©‰ã€‘
        ï¼ˆç”¨æ›´å©‰è½‰çš„æ–¹å¼è¡¨é”ï¼‰

        âœ… ç‰ˆæœ¬5ã€æ›´ç©æ¥µã€‘
        ï¼ˆå±•ç¾æ›´å¤šä¿¡å¿ƒèˆ‡ç†±æƒ…ï¼‰

        âœ… ç‰ˆæœ¬6ã€æ›´è©³ç´°ã€‘
        ï¼ˆæä¾›æ›´å¤šå…·é«”è³‡è¨Šï¼‰

        æ¯å€‹ç‰ˆæœ¬éƒ½è¦èƒ½ç›´æ¥è¤‡è£½ä½¿ç”¨ï¼
        """)
        
        last_prompt['task_description'] = task_description
        
        chain = prompt_template | self.llm
        result = chain.invoke(last_prompt)
        
        formatted_output = "ğŸ”„ æ›´å¤šå›è¦†é¸é …ï¼š\n\n"
        formatted_output += "=" * 40 + "\n"
        formatted_output += result.content.strip()
        formatted_output += "\n" + "=" * 40
        formatted_output += "\n\nğŸ’¡ é‚„éœ€è¦æ›´å¤šï¼Ÿå†è¼¸å…¥ /more"
        
        return formatted_output