import os
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()

class ReplyGenerator:
    """ç›´æ¥ç”Ÿæˆå¯ç”¨å›è¦†æ–‡å­—çš„è™•ç†å™¨"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            temperature=0.7,
            model="gpt-3.5-turbo",
            openai_api_key=os.getenv('OPENAI_API_KEY')
        )
    
    def generate_reply_options(self, context_data):
        """ç”Ÿæˆ3å€‹ä¸åŒé¢¨æ ¼çš„å›è¦†é¸é …"""
        
        prompt_template = ChatPromptTemplate.from_template("""
        ä½ æ˜¯å›è¦†å»ºè­°åŠ©æ‰‹ã€‚è«‹æ ¹æ“šç”¨æˆ¶æƒ…å¢ƒï¼Œç›´æ¥æä¾›3å€‹å¯ä»¥è¤‡è£½ä½¿ç”¨çš„å›è¦†æ–‡å­—ã€‚

        æƒ…å¢ƒè³‡è¨Šï¼š
        - èº«ä»½ï¼š{user_identity}
        - å°è±¡ï¼š{target_identity}
        - æƒ…å¢ƒï¼š{context}
        - æºé€šæ–¹å¼ï¼š{medium}
        - å…¬å¸æ–‡åŒ–ï¼š{culture}

        è¼¸å‡ºæ ¼å¼è¦æ±‚ï¼š
        1. åªæä¾›å›è¦†æ–‡å­—ï¼Œä¸è¦å°è©±éç¨‹
        2. æ¯å€‹é¸é …éƒ½æ˜¯å®Œæ•´ã€å¯ç›´æ¥ä½¿ç”¨çš„è¨Šæ¯
        3. ä¸è¦åŒ…å«"æˆ‘ï¼š"æˆ–èªªè©±è€…æ¨™ç±¤
        4. æ ¹æ“šæºé€šæ–¹å¼èª¿æ•´ï¼ˆLINEå¯ç”¨è¡¨æƒ…ã€Emailè¦å®Œæ•´ï¼‰

        è«‹æä¾›3å€‹é¸é …ï¼š

        ã€é¸é …1-æ­£å¼å§”å©‰ã€‘
        [æä¾›30-80å­—çš„å®Œæ•´å›è¦†ï¼Œé©åˆæ­£å¼å ´åˆ]

        ã€é¸é …2-å¹³è¡¡é©ä¸­ã€‘
        [æä¾›30-80å­—çš„å®Œæ•´å›è¦†ï¼Œå…¼é¡§ç¦®è²Œèˆ‡è¦ªå’Œ]

        ã€é¸é …3-è¼•é¬†ç›´æ¥ã€‘
        [æä¾›30-80å­—çš„å®Œæ•´å›è¦†ï¼Œè¼ƒå£èªåŒ–{emoji_hint}]

        æ³¨æ„ï¼š
        - ä½¿ç”¨ç¹é«”ä¸­æ–‡
        - ç¬¦åˆå°ç£ç”¨èªç¿’æ…£
        - æ¯å€‹é¸é …éƒ½è¦èƒ½ç›´æ¥è¤‡è£½ä½¿ç”¨
        """)
        
        # æ ¹æ“šåª’ä»‹æ±ºå®šæ˜¯å¦ä½¿ç”¨è¡¨æƒ…
        emoji_hint = "ï¼Œå¯é©åº¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿ" if context_data.get('medium') == 'LINE' else ""
        
        chain = prompt_template | self.llm
        
        params = {
            'user_identity': context_data.get('user_identity', 'ä¸€èˆ¬å“¡å·¥'),
            'target_identity': context_data.get('target_identity', 'ä¸»ç®¡'),
            'context': context_data.get('context', ''),
            'medium': context_data.get('medium', 'LINE'),
            'culture': context_data.get('culture', 'ä¸€èˆ¬'),
            'emoji_hint': emoji_hint
        }
        
        result = chain.invoke(params)
        return self._parse_reply_options(result.content)
    
    def _parse_reply_options(self, content):
        """è§£æç”Ÿæˆçš„å›è¦†é¸é …"""
        options = []
        
        # åˆ†å‰²é¸é …
        sections = content.split('ã€é¸é …')
        
        for section in sections[1:]:  # è·³éç¬¬ä¸€å€‹ç©ºç™½éƒ¨åˆ†
            if 'ã€‘' in section:
                title_end = section.index('ã€‘')
                title = section[:title_end]
                text = section[title_end+1:].strip()
                
                # æ¸…ç†æ–‡å­—
                text = text.replace('[', '').replace(']', '')
                text = text.split('\n')[0].strip()  # åªå–ç¬¬ä¸€è¡Œ
                
                # åˆ¤æ–·é¢¨æ ¼
                if 'æ­£å¼' in title or 'å§”å©‰' in title:
                    style = 'formal'
                    emoji = 'ğŸ‘”'
                elif 'å¹³è¡¡' in title or 'é©ä¸­' in title:
                    style = 'balanced'
                    emoji = 'ğŸ¤'
                else:
                    style = 'casual'
                    emoji = 'ğŸ˜Š'
                
                options.append({
                    'style': style,
                    'emoji': emoji,
                    'title': f"é¸é …{len(options)+1}ï¼š{title.strip('-')}",
                    'text': text
                })
        
        # å¦‚æœè§£æå¤±æ•—ï¼Œè¿”å›é è¨­é¸é …
        if not options:
            options = [
                {
                    'style': 'formal',
                    'emoji': 'ğŸ‘”',
                    'title': 'é¸é …1ï¼šæ­£å¼ç‰ˆ',
                    'text': content.strip()[:100]
                }
            ]
        
        return options
    
    def generate_quick_scenario_reply(self, scenario):
        """é‡å°å¿«é€Ÿæƒ…å¢ƒç”Ÿæˆå›è¦†"""
        
        quick_scenarios = {
            "è«‹å‡": {
                "context": "éœ€è¦è«‹å‡",
                "examples": [
                    "è€é—†æ—©å®‰ï¼Œæ˜å¤©éœ€è¦è«‹å‡ä¸€å¤©ï¼Œå®¶è£¡æœ‰æ€¥äº‹è¦è™•ç†",
                    "ä¸å¥½æ„æ€ï¼Œæ˜å¤©æƒ³è«‹å€‹å‡ï¼Œæœ‰äº›ç§äº‹éœ€è¦è™•ç†",
                    "è€é—†ï¼Œæ˜å¤©æœ‰äº‹æƒ³è«‹å‡ï¼Œæœƒå…ˆæŠŠå·¥ä½œå®‰æ’å¥½"
                ]
            },
            "æ‹’çµ•åŠ ç­": {
                "context": "å©‰æ‹’åŠ ç­è¦æ±‚",
                "examples": [
                    "ä¸å¥½æ„æ€ï¼Œä»Šæ™šå·²æœ‰å®‰æ’ï¼Œæ˜å¤©ä¸€æ—©æˆ‘æœƒå„ªå…ˆè™•ç†",
                    "æŠ±æ­‰ï¼Œæ™šä¸Šæœ‰äº‹èµ°ä¸é–‹ï¼Œé€™å€‹æˆ‘æ˜å¤©ç¬¬ä¸€ä»¶è™•ç†å¯ä»¥å—",
                    "ä»Šå¤©çœŸçš„ä¸è¡Œï¼Œå®¶è£¡æœ‰äº‹ğŸ˜… æ˜å¤©æˆ‘æ—©é»ä¾†è¶•"
                ]
            },
            "å‚¬é€²åº¦": {
                "context": "ç¦®è²Œå‚¬ä¿ƒé€²åº¦",
                "examples": [
                    "è«‹å•ä¹‹å‰æåˆ°çš„è³‡æ–™æº–å‚™å¥½äº†å—ï¼Ÿéœ€è¦çš„è©±æˆ‘å¯ä»¥å”åŠ©",
                    "ä¸å¥½æ„æ€æé†’ä¸€ä¸‹ï¼Œé‚£ä»½æ–‡ä»¶ä»Šå¤©éœ€è¦ç”¨åˆ°ï¼Œæ–¹ä¾¿äº†å—",
                    "Hiï¼Œä¸Šæ¬¡èªªçš„æ±è¥¿å¥½äº†å—ï¼Ÿè€é—†åœ¨å•ğŸ˜…"
                ]
            },
            "é“æ­‰": {
                "context": "å·¥ä½œå¤±èª¤é“æ­‰",
                "examples": [
                    "å¾ˆæŠ±æ­‰é€™æ¬¡çš„ç–å¤±ï¼Œæˆ‘æœƒç«‹å³ä¿®æ­£ä¸¦é¿å…å†æ¬¡ç™¼ç”Ÿ",
                    "ä¸å¥½æ„æ€ï¼Œæ˜¯æˆ‘çš„å¤±èª¤ï¼Œé¦¬ä¸Šè™•ç†ï¼Œä»¥å¾Œæœƒæ›´æ³¨æ„",
                    "æŠ±æ­‰æŠ±æ­‰ï¼Œæˆ‘çš„éŒ¯ğŸ’¦ ç¾åœ¨å°±æ”¹"
                ]
            }
        }
        
        if scenario in quick_scenarios:
            return quick_scenarios[scenario]["examples"]
        else:
            # ä½¿ç”¨ AI ç”Ÿæˆ
            context_data = {
                'context': scenario,
                'medium': 'LINE',
                'culture': 'ä¸€èˆ¬'
            }
            options = self.generate_reply_options(context_data)
            return [opt['text'] for opt in options]
    
    def adjust_tone(self, original_text, new_tone):
        """èª¿æ•´æ—¢æœ‰æ–‡å­—çš„èªæ°£"""
        
        prompt_template = ChatPromptTemplate.from_template("""
        è«‹å°‡ä»¥ä¸‹æ–‡å­—èª¿æ•´ç‚º{tone}çš„èªæ°£ï¼Œä¿æŒåŸæ„ä½†æ”¹è®Šè¡¨é”æ–¹å¼ï¼š

        åŸæ–‡ï¼š{original}

        èªæ°£èªªæ˜ï¼š
        - æ›´æ­£å¼ï¼šä½¿ç”¨æ•¬èªã€å®Œæ•´å¥å­ã€é¿å…å£èª
        - æ›´è¼•é¬†ï¼šåŠ å…¥å£èªã€é©åº¦è¡¨æƒ…ç¬¦è™Ÿã€è¦ªåˆ‡æ„Ÿ
        - æ›´å§”å©‰ï¼šé–“æ¥è¡¨é”ã€çµ¦å°æ–¹å°éšã€æŸ”å’Œç”¨è©
        - æ›´ç›´æ¥ï¼šç°¡æ½”æ˜ç­ã€ç›´èªªé‡é»ã€æ¸›å°‘ä¿®é£¾

        èª¿æ•´å¾Œï¼ˆç¹é«”ä¸­æ–‡ï¼‰ï¼š
        """)
        
        tone_map = {
            'formal': 'æ›´æ­£å¼',
            'casual': 'æ›´è¼•é¬†',
            'polite': 'æ›´å§”å©‰',
            'direct': 'æ›´ç›´æ¥'
        }
        
        chain = prompt_template | self.llm
        result = chain.invoke({
            'original': original_text,
            'tone': tone_map.get(new_tone, 'æ›´å¹³è¡¡')
        })
        
        return result.content.strip()